{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5de2004a-5231-4a73-8121-4829b7cb7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: requests\n",
      "Version: 2.32.3\n",
      "Summary: Python HTTP for Humans.\n",
      "Home-page: https://requests.readthedocs.io\n",
      "Author: Kenneth Reitz\n",
      "Author-email: me@kennethreitz.org\n",
      "License: Apache-2.0\n",
      "Location: C:\\Users\\hun\\anaconda3\\Lib\\site-packages\n",
      "Requires: certifi, charset-normalizer, idna, urllib3\n",
      "Required-by: aext-assistant-server, anaconda-catalogs, anaconda-client, anaconda-cloud-auth, anaconda-project, conda, conda-build, conda-repo-cli, conda_package_streaming, cookiecutter, datashader, jupyterlab_server, panel, requests-file, requests-toolbelt, Sphinx, streamlit, tldextract\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "97214439-be65-4b06-9077-dc5451b2876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: beautifulsoup4\n",
      "Version: 4.12.3\n",
      "Summary: Screen-scraping library\n",
      "Home-page: https://www.crummy.com/software/BeautifulSoup/bs4/\n",
      "Author: \n",
      "Author-email: Leonard Richardson <leonardr@segfault.org>\n",
      "License: MIT License\n",
      "Location: C:\\Users\\hun\\anaconda3\\Lib\\site-packages\n",
      "Requires: soupsieve\n",
      "Required-by: conda-build, nbconvert\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6571e16d-2295-44cc-aef7-32d5f8139485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reqeusts, bs4 import\n",
    "import requests\n",
    "import bs4\n",
    "# BeautifulSoup 클래스 import\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "007d5514-a9e7-4b99-a16a-62d9babd99b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests 버전 2.32.3\n",
      "beautifulsoup4 버전 4.12.3\n"
     ]
    }
   ],
   "source": [
    "# requests, bs4 버전 확인하기\n",
    "print(f'requests 버전 {requests.__version__}')\n",
    "print(f'beautifulsoup4 버전 {bs4.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fae2baab-c168-4154-a495-08659ad7c4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요청 URL: https://www.chosun.com\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# IT/과학 뉴스\n",
    "req_param = {\n",
    "    'sid': 105\n",
    "}\n",
    "\n",
    "# Naver 뉴스 IT/과학 섹션 URL\n",
    "url = 'https://news.naver.com/section/{sid}'.format(**req_param)\n",
    "\n",
    "print(\"요청 URL:\", url)\n",
    "\n",
    "# 요청 헤더 설정 : 브라우저 정보 흉내내기\n",
    "req_header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# GET 요청 보내기\n",
    "response = requests.get(url, headers=req_header)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    # 응답에서 HTML 추출\n",
    "    html = response.text\n",
    "\n",
    "    # BeautifulSoup 객체 생성\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 뉴스 링크 찾기 (모바일 뉴스 article 링크 포함된 <a> 태그들)\n",
    "    links = soup.select(\"div.sa_text a[href*='mnews/article']\")\n",
    "    \n",
    "    # 결과 출력\n",
    "    for link in links:\n",
    "        print(link.text.strip())\n",
    "        print(\"기사 링크:\", link['href'])\n",
    "        print('-' * 40)\n",
    "else:\n",
    "    print(\"요청 실패, 상태코드:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "75043aca-e1ae-4a36-b5e0-7034655cbffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📢 [생활/문화] 뉴스 목록\n",
      "----------------------------------------\n",
      "제21대 대통령 선거일 '6월 3일 화요일' 확정…임시공휴일 지정\n",
      "링크: https://n.news.naver.com/mnews/article/055/0001247361\n",
      "------------------------------\n",
      "\n",
      "링크: https://n.news.naver.com/mnews/article/comment/055/0001247361\n",
      "------------------------------\n",
      "국민의힘 \"이재명 개헌 거부, 시대적 요구 외면하는 것\"\n",
      "링크: https://n.news.naver.com/mnews/article/055/0001247151\n",
      "------------------------------\n",
      "\n",
      "링크: https://n.news.naver.com/mnews/article/comment/055/0001247151\n",
      "------------------------------\n",
      "홍준표 \"탄핵되면 차기 대선 포기해야···당이 없어지는 거지\" 2024년 9월 'TV홍카콜라' 다시 봤더니\n",
      "링크: https://n.news.naver.com/mnews/article/657/0000037003\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "section_dict = {\n",
    "    100: '정치',\n",
    "    101: '경제',\n",
    "    102: '사회',\n",
    "    103: '생활/문화',\n",
    "    104: '세계',\n",
    "    105: 'IT/과학'\n",
    "}\n",
    "\n",
    "def print_news(sid, section):  # 예: print_news(103, '생활/문화')\n",
    "    print(f\"📢 [{section}] 뉴스 목록\")\n",
    "    print('-' * 40)\n",
    "\n",
    "    url = f'https://news.naver.com/section/{sid}'\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.select(\"div.sa_text a[href*='mnews/article']\")\n",
    "\n",
    "        for link in links[:5]:  # 상위 5개만 출력\n",
    "            print(link.text.strip())\n",
    "            print(\"링크:\", link['href'])\n",
    "            print('-' * 30)\n",
    "    else:\n",
    "        print(\"❌ 요청 실패:\", response.status_code)\n",
    "\n",
    "# 함수 호출\n",
    "print_news(100, section_dict[103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c85517-6b34-4f8a-abd2-616a1eebc273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
